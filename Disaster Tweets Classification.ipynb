{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":865552,"sourceType":"datasetVersion","datasetId":459672},{"sourceId":1483651,"sourceType":"datasetVersion","datasetId":870709}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:35.235749Z","iopub.execute_input":"2025-04-01T04:31:35.235969Z","iopub.status.idle":"2025-04-01T04:31:36.244481Z","shell.execute_reply.started":"2025-04-01T04:31:35.235948Z","shell.execute_reply":"2025-04-01T04:31:36.243637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndata=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndata2 = pd.read_csv('/kaggle/input/disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv')[[\"keyword\",\"location\",\"text\",\"choose_one\"]]\ndata2.rename(columns={\"choose_one\":\"target\"},inplace=True)\ndata2[\"target\"] = (data2[\"target\"] == \"Relevant\").astype(\"int\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:36.245374Z","iopub.execute_input":"2025-04-01T04:31:36.245837Z","iopub.status.idle":"2025-04-01T04:31:36.465733Z","shell.execute_reply.started":"2025-04-01T04:31:36.245806Z","shell.execute_reply":"2025-04-01T04:31:36.464957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.columns)\nprint(data2.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:39.979016Z","iopub.execute_input":"2025-04-01T04:31:39.979326Z","iopub.status.idle":"2025-04-01T04:31:39.984669Z","shell.execute_reply.started":"2025-04-01T04:31:39.979299Z","shell.execute_reply":"2025-04-01T04:31:39.983795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Concatenating two datsets ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming data and data2 are already loaded as pandas DataFrames\n\n# Selecting only 'text' and 'target' columns from both dataframes\ndata = pd.concat([data[['text', 'target']], data2[['text', 'target']]], ignore_index=True)\n\n# Display the first few rows\ndata\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:40.444689Z","iopub.execute_input":"2025-04-01T04:31:40.445082Z","iopub.status.idle":"2025-04-01T04:31:40.478893Z","shell.execute_reply.started":"2025-04-01T04:31:40.44505Z","shell.execute_reply":"2025-04-01T04:31:40.478028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data.isnull().sum())  # Check for missing values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:40.590235Z","iopub.execute_input":"2025-04-01T04:31:40.590549Z","iopub.status.idle":"2025-04-01T04:31:40.598058Z","shell.execute_reply.started":"2025-04-01T04:31:40.590525Z","shell.execute_reply":"2025-04-01T04:31:40.597342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom sklearn.model_selection import train_test_split\nimport re\n# Download NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Initialize stemmer and stopwords\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\n\n\ndef preprocess_text(text):\n    # Lowercase the text\n    text = text.lower()\n\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+', '', text)  # Removes anything starting with 'http' or 'www'\n\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n\n    # Tokenize the text\n    tokens = word_tokenize(text)\n\n    # Join the tokens back into a single string\n    return ' '.join(tokens)\n\n# Apply preprocessing to the 'text' column\ndata['processed_text'] = data['text'].apply(preprocess_text)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data['processed_text'], data['target'], test_size=0.2, random_state=42)\n\n# Display the processed data\nX_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:41.931876Z","iopub.execute_input":"2025-04-01T04:31:41.932143Z","iopub.status.idle":"2025-04-01T04:31:45.252099Z","shell.execute_reply.started":"2025-04-01T04:31:41.932122Z","shell.execute_reply":"2025-04-01T04:31:45.251134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd  \n\n# Assuming y contains class labels\nprint(y_train.value_counts())  \n\n# To get percentages\nprint(y_train.value_counts(normalize=True) * 100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:47.298019Z","iopub.execute_input":"2025-04-01T04:31:47.298462Z","iopub.status.idle":"2025-04-01T04:31:47.309957Z","shell.execute_reply.started":"2025-04-01T04:31:47.298434Z","shell.execute_reply":"2025-04-01T04:31:47.308811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train[:1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:48.472899Z","iopub.execute_input":"2025-04-01T04:31:48.473184Z","iopub.status.idle":"2025-04-01T04:31:48.479087Z","shell.execute_reply.started":"2025-04-01T04:31:48.473161Z","shell.execute_reply":"2025-04-01T04:31:48.478329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(X_train[:1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:48.852678Z","iopub.execute_input":"2025-04-01T04:31:48.852957Z","iopub.status.idle":"2025-04-01T04:31:48.858437Z","shell.execute_reply.started":"2025-04-01T04:31:48.852935Z","shell.execute_reply":"2025-04-01T04:31:48.857701Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Convert text to TF-IDF features\nvectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:50.031798Z","iopub.execute_input":"2025-04-01T04:31:50.032099Z","iopub.status.idle":"2025-04-01T04:31:50.304443Z","shell.execute_reply.started":"2025-04-01T04:31:50.032075Z","shell.execute_reply":"2025-04-01T04:31:50.30375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Train the model\nmodel_tfidf = LogisticRegression()\nmodel_tfidf.fit(X_train_tfidf, y_train)\n\n# Evaluate the model\ny_pred = model_tfidf.predict(X_test_tfidf)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:31:50.305448Z","iopub.execute_input":"2025-04-01T04:31:50.305669Z","iopub.status.idle":"2025-04-01T04:31:50.465064Z","shell.execute_reply.started":"2025-04-01T04:31:50.30565Z","shell.execute_reply":"2025-04-01T04:31:50.464096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Find misclassified indices\nmisclassified_indices = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n\n# Extract misclassified examples\nmisclassified_data = pd.DataFrame({\n    \"Text\": X_test.iloc[misclassified_indices],\n    \"True Label\": y_test.iloc[misclassified_indices],\n    \"Predicted Label\": y_pred[misclassified_indices]\n})\n\n# Display misclassified examples\nprint(\"\\nMisclassified Examples:\")\nprint(misclassified_data.head(10))  # Show first 10 misclassified examples\n\n# Analyze feature importance\nfeature_names = vectorizer.get_feature_names_out()\ncoef = model_tfidf.coef_[0]  # Coefficients of logistic regression\n\n# Find top positive and negative words\ntop_positive_words = [feature_names[i] for i in coef.argsort()[-10:]]  # 10 words most strongly predicting positive class\ntop_negative_words = [feature_names[i] for i in coef.argsort()[:10]]   # 10 words most strongly predicting negative class\n\nprint(\"\\nTop Words Indicating Positive Class:\", top_positive_words)\nprint(\"Top Words Indicating Negative Class:\", top_negative_words)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:32:09.4162Z","iopub.execute_input":"2025-04-01T04:32:09.416509Z","iopub.status.idle":"2025-04-01T04:32:09.430837Z","shell.execute_reply.started":"2025-04-01T04:32:09.416485Z","shell.execute_reply":"2025-04-01T04:32:09.429954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reasons for misclassification\n1. Loss of Context & Word Order\n2. Failure to Capture Semantic Meaning\n3. Presence of Ambiguous or Common Words","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# Download NLTK data (if not already downloaded)\nnltk.download('punkt')\n\n# Tokenize the text\nX_train_tokens = [word_tokenize(text) for text in X_train]\nX_test_tokens = [word_tokenize(text) for text in X_test]\n\nfrom gensim.models import KeyedVectors\n\n# Load pre-trained Word2Vec embeddings (e.g., Google News)\nword2vec_model = KeyedVectors.load_word2vec_format('/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\n\n# Save the model as KeyedVectors\nword2vec_model.save(\"word2vec.model\")\n\n# Load the model as KeyedVectors\nword2vec_model = KeyedVectors.load(\"word2vec.model\", mmap='r')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:32:12.87194Z","iopub.execute_input":"2025-04-01T04:32:12.872256Z","iopub.status.idle":"2025-04-01T04:33:22.903766Z","shell.execute_reply.started":"2025-04-01T04:32:12.872229Z","shell.execute_reply":"2025-04-01T04:33:22.902777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef sentence_vector(tokens, model, vector_size=300):  # Set vector_size to match the embedding dimension \n    vec = np.zeros(vector_size)  # Initialize a vector of zeros\n    count = 0  # Count of words with valid vectors\n    for word in tokens:\n        if word in model:  # Check if the word is in the KeyedVectors vocabulary\n            vec += model[word]  # Add the word vector\n            count += 1\n    if count != 0:\n        vec /= count  # Average the vectors\n    return vec\n\n# Convert tokenized sentences to sentence vectors\nX_train_vectors = np.array([sentence_vector(tokens, word2vec_model) for tokens in X_train_tokens])\nX_test_vectors = np.array([sentence_vector(tokens, word2vec_model) for tokens in X_test_tokens])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:33:22.904891Z","iopub.execute_input":"2025-04-01T04:33:22.905211Z","iopub.status.idle":"2025-04-01T04:33:24.281592Z","shell.execute_reply.started":"2025-04-01T04:33:22.905179Z","shell.execute_reply":"2025-04-01T04:33:24.280834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Train the model\nmodel_w2v = LogisticRegression()\nmodel_w2v.fit(X_train_vectors, y_train)\n\n# Evaluate the model\ny_pred_w2v = model_w2v.predict(X_test_vectors)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_w2v))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_w2v))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:33:24.282998Z","iopub.execute_input":"2025-04-01T04:33:24.283299Z","iopub.status.idle":"2025-04-01T04:33:28.767177Z","shell.execute_reply.started":"2025-04-01T04:33:24.283261Z","shell.execute_reply":"2025-04-01T04:33:28.766389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset index to ensure consistent integer indexing\nX_test = X_test.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)\n\n# Ensure predictions are also converted to arrays if needed\ny_pred = np.array(y_pred)\ny_pred_w2v = np.array(y_pred_w2v)\n\n# Debugging: Check what indices exist in X_test\nvalid_indices = set(X_test.index)\ni=0\n# Print the misclassified samples with their true labels and predictions\nfor idx in wrong_tfidf_correct_w2v:\n    if(i>2): break\n    if idx in valid_indices:\n        print(f\"Sample Sentence: {X_test.iloc[idx]}\")\n        print(f\"True Label: {y_test.iloc[idx]}, TF-IDF Prediction: {y_pred[idx]}, W2V Prediction: {y_pred_w2v[idx]}\\n\")\n    else:\n        print(f\"Warning: Index {idx} not found in X_test. Skipping...\")\n    i+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:37:12.413434Z","iopub.execute_input":"2025-04-01T04:37:12.413764Z","iopub.status.idle":"2025-04-01T04:37:12.421864Z","shell.execute_reply.started":"2025-04-01T04:37:12.413737Z","shell.execute_reply":"2025-04-01T04:37:12.42102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Create a DataFrame for easier analysis\nmisclassified_df = pd.DataFrame({\n    'Text': X_test, \n    'True Label': y_test, \n    'Predicted Label': y_pred\n})\n\n# Filter only misclassified examples\nmisclassified_df = misclassified_df[misclassified_df['True Label'] != misclassified_df['Predicted Label']]\n\n# Display misclassified examples\nprint(\"Misclassified Examples:\")\nprint(misclassified_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:37:42.577187Z","iopub.execute_input":"2025-04-01T04:37:42.577546Z","iopub.status.idle":"2025-04-01T04:37:42.586731Z","shell.execute_reply.started":"2025-04-01T04:37:42.577515Z","shell.execute_reply":"2025-04-01T04:37:42.585892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reasons for misclassification\n1. Failure to Capture Sentence Context \n2. Vocabulary Coverage & OOV (Out-of-Vocabulary) Words\n3. Does Not Account for Sentence Structure","metadata":{}},{"cell_type":"code","source":"!!pip install wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:37:45.845095Z","iopub.execute_input":"2025-04-01T04:37:45.845402Z","iopub.status.idle":"2025-04-01T04:37:49.96529Z","shell.execute_reply.started":"2025-04-01T04:37:45.845377Z","shell.execute_reply":"2025-04-01T04:37:49.964565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"4bf281fc3d5b1a088a9793a64f35253a6c2d7bc6\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:37:49.966231Z","iopub.execute_input":"2025-04-01T04:37:49.966576Z","iopub.status.idle":"2025-04-01T04:37:58.588689Z","shell.execute_reply.started":"2025-04-01T04:37:49.966553Z","shell.execute_reply":"2025-04-01T04:37:58.587935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:37:58.590193Z","iopub.execute_input":"2025-04-01T04:37:58.590701Z","iopub.status.idle":"2025-04-01T04:38:02.155091Z","shell.execute_reply.started":"2025-04-01T04:37:58.590676Z","shell.execute_reply":"2025-04-01T04:38:02.153973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n\n# Load BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_model.to(device)  # Move model to GPU if available\n\n# Function to extract BERT embeddings using batch processing\ndef get_bert_embeddings(text_list, batch_size=16):\n    embeddings = []\n    \n    # Ensure all inputs are strings\n    text_list = [str(text) for text in text_list]\n\n    for i in tqdm(range(0, len(text_list), batch_size), desc=\"Extracting BERT embeddings\"):\n        batch_texts = text_list[i:i + batch_size]\n        inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            outputs = bert_model(**inputs)\n        \n        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Get CLS token\n        embeddings.append(batch_embeddings)\n    \n    return np.vstack(embeddings)  # Stack all embeddings into a single array\n\n# Convert data to lists of strings\nX_train = [str(x) for x in X_train]  # Convert each element to string\nX_test = [str(x) for x in X_test]    # Convert each element to string\n\n# Extract BERT embeddings\nX_train_embedded = get_bert_embeddings(X_train, batch_size=16)\nX_test_embedded = get_bert_embeddings(X_test, batch_size=16)\n\n# Train Logistic Regression Model\nlogreg = LogisticRegression(max_iter=500)\nlogreg.fit(X_train_embedded, y_train)\n\n# Make pre_dictions\ny_pred_bemb = logreg.predict(X_test_embedded)\n\n# Convert y_test to NumPy array to avoid KeyError\ny_test = np.array(y_test)\n\n# Calculate Accuracy\naccuracy = accuracy_score(y_test, y_pred_bemb)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:38:02.156347Z","iopub.execute_input":"2025-04-01T04:38:02.156624Z","iopub.status.idle":"2025-04-01T04:38:59.668635Z","shell.execute_reply.started":"2025-04-01T04:38:02.156601Z","shell.execute_reply":"2025-04-01T04:38:59.66766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make pre_dictions\ny_pred_bemb = logreg.predict(X_test_embedded)\n\n# Convert y_test to NumPy array to avoid KeyError\ny_test = np.array(y_test)\n\n# Calculate Accuracy\naccuracy = accuracy_score(y_test, y_pred_bemb)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Convert predictions and y_test to NumPy arrays if not already\ny_pred_w2v = np.array(y_pred_w2v)\ny_pred_bemb = np.array(y_pred_bemb)\ny_test = np.array(y_test)\n\n# Find indices where Word2Vec was incorrect, but Logistic Regression was correct\nwrong_w2v_correct_bemb = [\n    idx for idx in range(len(y_test)) if (y_pred_w2v[idx] != y_test[idx]) and (y_pred_bemb[idx] == y_test[idx])\n]\n\n# Print the misclassified samples\nprint(\"Samples misclassified by Word2Vec but classified correctly by Logistic Regression:\\n\")\nfor idx in wrong_w2v_correct_bemb[:5]:  # Limit output to 5 samples for readability\n    print(f\"Sample Sentence: {X_test[idx]}\")  # Use direct indexing\n    print(f\"True Label: {y_test[idx]}, W2V Prediction: {y_pred_w2v[idx]}, BERT/Embeddings Prediction: {y_pred_bemb[idx]}\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:53:21.986606Z","iopub.execute_input":"2025-04-01T04:53:21.986934Z","iopub.status.idle":"2025-04-01T04:53:22.003279Z","shell.execute_reply.started":"2025-04-01T04:53:21.986909Z","shell.execute_reply":"2025-04-01T04:53:22.002511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# Define accuracy metric\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_score(labels, predictions)\n    return {\"accuracy\": accuracy}\n\n# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Ensure X_train and X_test are lists of strings\nX_train = [str(text) for text in X_train]\nX_test = [str(text) for text in X_test]\n\n# Tokenize the data\ntrain_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=512)\ntest_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=512)\n\n# Ensure labels are lists\ny_train = list(y_train)\ny_test = list(y_test)\n\n# Custom PyTorch dataset class\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Convert labels to LongTensor\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\n# Create datasets\ntrain_dataset = Dataset(train_encodings, y_train)\ntest_dataset = Dataset(test_encodings, y_test)\n\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=10,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",  # Use accuracy to pick best model\n    greater_is_better=True,  # Higher accuracy is better\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,  # Add accuracy metric\n)\n\n# Train and evaluate the model\ntrainer.train()\ntrainer.evaluate()\n\n# Save the trained model and tokenizer\nmodel.save_pretrained(\"./bert_text_classifier\")\ntokenizer.save_pretrained(\"./bert_text_classifier\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:04:32.06383Z","iopub.execute_input":"2025-04-01T04:04:32.064261Z","iopub.status.idle":"2025-04-01T04:22:40.837595Z","shell.execute_reply.started":"2025-04-01T04:04:32.064228Z","shell.execute_reply":"2025-04-01T04:22:40.836861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get model predictions\npredictions = trainer.predict(test_dataset).predictions\npredicted_labels = np.argmax(predictions, axis=-1)\n\n# Identify misclassified samples\nmisclassified_indices = np.where(predicted_labels != np.array(y_test))[0]\n\nprint(\"\\nMisclassified Samples:\")\nfor idx in misclassified_indices:\n    print(\"-\" * 50)\n    print(f\"Sample {idx+1}:\")\n    print(f\"  Text: {X_test[idx]}\")\n    print(f\"  True Label: {y_test[idx]}\")\n    print(f\"  Predicted Label: {predicted_labels[idx]}\")\nprint(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T04:22:56.791217Z","iopub.execute_input":"2025-04-01T04:22:56.791553Z","iopub.status.idle":"2025-04-01T04:23:03.122599Z","shell.execute_reply.started":"2025-04-01T04:22:56.791526Z","shell.execute_reply":"2025-04-01T04:23:03.121862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}